#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scrapbox API Proxy Server
CORSåˆ¶é™ã‚’å›é¿ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒ
"""

import json
import urllib.request
import urllib.parse
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.error import HTTPError, URLError

class ScrapboxProxyHandler(BaseHTTPRequestHandler):
    def do_OPTIONS(self):
        """CORS preflight request ã«å¯¾å¿œ"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')
        self.send_header('Access-Control-Max-Age', '86400')
        self.end_headers()
        
    def do_GET(self):
        """GET request ã‚’å‡¦ç†"""
        try:
            # ãƒ‘ã‚¹ã‚’è§£æ
            path = self.path
            print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ã‚¹: {path}")
            
            if path.startswith('/api/'):
                # /api/page-data/export/{project}.json -> https://scrapbox.io/api/page-data/export/{project}.json
                scrapbox_url = f"https://scrapbox.io{path}"
                print(f"Scrapbox URL: {scrapbox_url}")
                
                # Scrapbox APIã¸ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                response = self.fetch_from_scrapbox(scrapbox_url)
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹é€ä¿¡
                self.send_response(200)
                self.send_header('Access-Control-Allow-Origin', '*')
                self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
                self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')
                self.send_header('Access-Control-Max-Age', '86400')
                self.send_header('Content-Type', 'application/json; charset=utf-8')
                self.end_headers()
                
                self.wfile.write(response.encode('utf-8'))
                
            else:
                # ä¸æ­£ãªãƒ‘ã‚¹
                self.send_error(404, "Not Found")
                
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
            self.send_error(500, str(e))
    
    def fetch_from_scrapbox(self, url):
        """Scrapbox APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (cosense-taskflow proxy)',
                'Accept': 'application/json'
            }
            
            # ãƒ‡ãƒãƒƒã‚°: å—ä¿¡ã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã™ã¹ã¦è¡¨ç¤º
            print("ğŸ“¨ å—ä¿¡ãƒ˜ãƒƒãƒ€ãƒ¼:")
            for header_name, header_value in self.headers.items():
                print(f"   {header_name}: {header_value}")
            
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®Authorizationãƒ˜ãƒƒãƒ€ãƒ¼ã‚’Cookieã¨ã—ã¦è»¢é€
            auth_header = self.headers.get('Authorization')
            print(f"ğŸ” Authorization ãƒ˜ãƒƒãƒ€ãƒ¼: {auth_header}")
            
            if auth_header and auth_header.startswith('Bearer '):
                cookie_value = auth_header[7:]  # "Bearer " ã‚’é™¤å»
                headers['Cookie'] = f'connect.sid={cookie_value}'
                print(f"ğŸ” Cookieèªè¨¼ã‚’ä½¿ç”¨:")
                print(f"   Cookieå€¤: {cookie_value}")
                print(f"   é€ä¿¡URL: {url}")
            else:
                print("ğŸ”“ èªè¨¼ãªã—ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
                print(f"   ç†ç”±: auth_header='{auth_header}'")
            
            req = urllib.request.Request(url, headers=headers)
            
            with urllib.request.urlopen(req, timeout=30) as response:
                data = response.read().decode('utf-8')
                print(f"å–å¾—å®Œäº†: {len(data)} bytes")
                return data
                
        except HTTPError as e:
            error_msg = f"HTTP Error {e.code}: {e.reason}"
            print(error_msg)
            raise Exception(error_msg)
            
        except URLError as e:
            error_msg = f"URL Error: {e.reason}"
            print(error_msg)
            raise Exception(error_msg)
            
        except Exception as e:
            error_msg = f"Request Error: {str(e)}"
            print(error_msg)
            raise Exception(error_msg)
    
    def log_message(self, format, *args):
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º"""
        print(f"[{self.address_string()}] {format % args}")

def run_proxy_server(port=8080):
    """ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ã‚’é–‹å§‹"""
    server_address = ('localhost', port)
    httpd = HTTPServer(server_address, ScrapboxProxyHandler)
    
    print(f"Scrapbox APIãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
    print(f"ãƒãƒ¼ãƒˆ: {port}")
    print(f"URL: http://localhost:{port}")
    print(f"ä¾‹: http://localhost:{port}/api/pages/your-project")
    print("Ctrl+C ã§åœæ­¢ã—ã¾ã™")
    
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        print("\nã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢ã—ã¦ã„ã¾ã™...")
        httpd.server_close()
        print("ã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    import sys
    
    port = 8080
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            print("ç„¡åŠ¹ãªãƒãƒ¼ãƒˆç•ªå·ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®8080ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    run_proxy_server(port)